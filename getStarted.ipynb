{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inciando a sessão spark, que pode ser vista como uma manifestação do driver process para o usuário "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.getOrCreate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definindo a leitura do arquivo. \n",
    "A leitura de um arquivo, não e uma action dentro do spark, por conta disso, ela será apenas inserida a uma pilha de transformações, que serão todas executadas no momento em que uma action for invocada\n",
    "\n",
    "Interessante saber que, nesse momento meu df, possui um determinado numero de colunas, porém um indeterminado numero de linhas, isso se dá devido a ação de leitura ser executada em lazy evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Timestamp: double, Open: double, High: double, Low: double, Close: double, Volume: double]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_bitcoin = spark.read.option('inferSchema', 'true'\n",
    "                        ).option('header', 'true'\n",
    "                        ).csv('./data/archive/btcusd_1-min_data.csv')\n",
    "df_bitcoin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilizando a action take"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Timestamp=1325412060.0, Open=4.58, High=4.58, Low=4.58, Close=4.58, Volume=0.0),\n",
       " Row(Timestamp=1325412120.0, Open=4.58, High=4.58, Low=4.58, Close=4.58, Volume=0.0)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_bitcoin.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construindo nossa primeira transformação"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aqui, vou realizar uma transformação do tipo sort. \n",
    "Por definição a ordenação de uma linha em relação a outras não pode ser tratada como uma narrow transformation.\n",
    "Uma vez que a ordenação depende das outras linhas, por conta idsso trata-se de uma wide transformations. (quando o processamento é feito em disco)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilizando o explain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Sort [Timestamp#17 ASC NULLS FIRST], true, 0\n",
      "   +- Exchange rangepartitioning(Timestamp#17 ASC NULLS FIRST, 200), ENSURE_REQUIREMENTS, [plan_id=45]\n",
      "      +- FileScan csv [Timestamp#17,Open#18,High#19,Low#20,Close#21,Volume#22] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/a:/DS/Spark/learningSpark/data/archive/btcusd_1-min_data.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<Timestamp:double,Open:double,High:double,Low:double,Close:double,Volume:double>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_bitcoin.sort('Timestamp').explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternado a configuração do spark, para retornar 5 arquivos de particao como saida "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set('spark.sql.shuffle.partitions', '5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Timestamp=None, Open=57854.0, High=57864.0, Low=57835.0, Close=57835.0, Volume=1.35346619),\n",
       " Row(Timestamp=1325412060.0, Open=4.58, High=4.58, Low=4.58, Close=4.58, Volume=0.0),\n",
       " Row(Timestamp=1325412120.0, Open=4.58, High=4.58, Low=4.58, Close=4.58, Volume=0.0),\n",
       " Row(Timestamp=1325412180.0, Open=4.58, High=4.58, Low=4.58, Close=4.58, Volume=0.0),\n",
       " Row(Timestamp=1325412240.0, Open=4.58, High=4.58, Low=4.58, Close=4.58, Volume=0.0),\n",
       " Row(Timestamp=1325412300.0, Open=4.58, High=4.58, Low=4.58, Close=4.58, Volume=0.0),\n",
       " Row(Timestamp=1325412360.0, Open=4.58, High=4.58, Low=4.58, Close=4.58, Volume=0.0),\n",
       " Row(Timestamp=1325412420.0, Open=4.58, High=4.58, Low=4.58, Close=4.58, Volume=0.0)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_bitcoin.sort('Timestamp').take(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrames and SQL\n",
    "\n",
    "O spark me permite registrar qualquer dataframe, como uma tabela, para então executar quaisuqer transformações/acoes nesse dataframe via puro sql. \n",
    "\n",
    "\n",
    "NÃO HÁ DIFERENÇAS DE PERFOMANCE ENTRE executar queries sql e exeuctar dataframe transformations. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bitcoin.createOrReplaceTempView('bictoin_data')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlWay = spark.sql(''' \n",
    "                    select \n",
    "                        to_date(FROM_UNIXTIME(Timestamp)) as data, \n",
    "                        count(*)\n",
    "                     from bictoin_data\n",
    "                     group by Timestamp\n",
    "                     order by Timestamp\n",
    "                   ''')\n",
    "dfWay = df_bitcoin.groupBy('Timestamp').count().sort('Timestamp') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Project [data#214, count(1)#216L]\n",
      "   +- Sort [Timestamp#17 ASC NULLS FIRST], true, 0\n",
      "      +- Exchange rangepartitioning(Timestamp#17 ASC NULLS FIRST, 5), ENSURE_REQUIREMENTS, [plan_id=763]\n",
      "         +- HashAggregate(keys=[Timestamp#17], functions=[count(1)])\n",
      "            +- Exchange hashpartitioning(Timestamp#17, 5), ENSURE_REQUIREMENTS, [plan_id=760]\n",
      "               +- HashAggregate(keys=[knownfloatingpointnormalized(normalizenanandzero(Timestamp#17)) AS Timestamp#17], functions=[partial_count(1)])\n",
      "                  +- FileScan csv [Timestamp#17] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/a:/DS/Spark/learningSpark/data/archive/btcusd_1-min_data.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<Timestamp:double>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlWay.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Sort [Timestamp#17 ASC NULLS FIRST], true, 0\n",
      "   +- Exchange rangepartitioning(Timestamp#17 ASC NULLS FIRST, 5), ENSURE_REQUIREMENTS, [plan_id=784]\n",
      "      +- HashAggregate(keys=[Timestamp#17], functions=[count(1)])\n",
      "         +- Exchange hashpartitioning(Timestamp#17, 5), ENSURE_REQUIREMENTS, [plan_id=781]\n",
      "            +- HashAggregate(keys=[knownfloatingpointnormalized(normalizenanandzero(Timestamp#17)) AS Timestamp#17], functions=[partial_count(1)])\n",
      "               +- FileScan csv [Timestamp#17] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/a:/DS/Spark/learningSpark/data/archive/btcusd_1-min_data.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<Timestamp:double>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfWay.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ao invocar um \"Take\" estou fazendo com que o Spark execute o plano de transformações. \n",
    "\n",
    "Essa estratégia seria interessante (utilizar takes) somente para monitrar se as transformações vem ocorrendo conforme o esperado\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(data=None, count(1)=1),\n",
       " Row(data=datetime.date(2012, 1, 1), count(1)=1),\n",
       " Row(data=datetime.date(2012, 1, 1), count(1)=1),\n",
       " Row(data=datetime.date(2012, 1, 1), count(1)=1),\n",
       " Row(data=datetime.date(2012, 1, 1), count(1)=1)]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sqlWay.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+\n",
      "|   Timestamp|count|\n",
      "+------------+-----+\n",
      "|        NULL|    1|\n",
      "|1.32541206E9|    1|\n",
      "|1.32541212E9|    1|\n",
      "|1.32541218E9|    1|\n",
      "|1.32541224E9|    1|\n",
      "| 1.3254123E9|    1|\n",
      "|1.32541236E9|    1|\n",
      "|1.32541242E9|    1|\n",
      "|1.32541248E9|    1|\n",
      "|1.32541254E9|    1|\n",
      "| 1.3254126E9|    1|\n",
      "|1.32541266E9|    1|\n",
      "|1.32541272E9|    1|\n",
      "|1.32541278E9|    1|\n",
      "|1.32541284E9|    1|\n",
      "| 1.3254129E9|    1|\n",
      "|1.32541296E9|    1|\n",
      "|1.32541302E9|    1|\n",
      "|1.32541308E9|    1|\n",
      "|1.32541314E9|    1|\n",
      "+------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfWay.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Brincando com manipulacoes um pouco mais complexas\n",
    "\n",
    "Quais sao as 5 primeiras datas em que o bitcoin teve sua maior alta?  e  sua maior baixa? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import from_unixtime, to_date\n",
    "\n",
    "top_five_highersSql = spark.sql(''' \n",
    "    select \n",
    "        to_date(FROM_UNIXTIME(Timestamp)) as data, \n",
    "        high as maior_alta\n",
    "\n",
    "    from bictoin_data \n",
    "    order by high desc   \n",
    "    limit 5  \n",
    "\n",
    "''')\n",
    "\n",
    "topFiveHighersDf = df_bitcoin.withColumn('data', to_date(from_unixtime(\"Timestamp\"))\n",
    "                                         ).select('data', 'high'\n",
    "                                         ).withColumnRenamed('high', 'maiorAlta'\n",
    "                                         ).orderBy('maiorAlta', ascending = False\n",
    "                                         ).limit(5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(data=datetime.date(2025, 1, 20), maiorAlta=109030.0),\n",
       " Row(data=datetime.date(2025, 1, 20), maiorAlta=109024.0),\n",
       " Row(data=datetime.date(2025, 1, 20), maiorAlta=108979.0),\n",
       " Row(data=datetime.date(2025, 1, 20), maiorAlta=108939.0),\n",
       " Row(data=datetime.date(2025, 1, 20), maiorAlta=108908.0)]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topFiveHighersDf.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(data=datetime.date(2025, 1, 20), maior_alta=109030.0),\n",
       " Row(data=datetime.date(2025, 1, 20), maior_alta=109024.0),\n",
       " Row(data=datetime.date(2025, 1, 20), maior_alta=108979.0),\n",
       " Row(data=datetime.date(2025, 1, 20), maior_alta=108939.0),\n",
       " Row(data=datetime.date(2025, 1, 20), maior_alta=108908.0)]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_five_highersSql.take(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
